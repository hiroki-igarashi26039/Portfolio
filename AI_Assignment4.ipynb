{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0d37628-d11d-43ca-bd70-44567aebbf41"
    }
   },
   "source": [
    "# 課題4: 評判分析\n",
    "\n",
    "本課題ではAmazonに投稿された映画のレビュー(英語)を分析し、レビューがPositiveかNegativeかの判別を行います。\n",
    "\n",
    "今回の課題のディレクトリには `Training_data` と `Test_data` が入っています。さらにその中には `pos` と `neg` というディレクトリがあり、Positiveなデータが `pos` に、Negativeなデータが `neg` に入っています。\n",
    "\n",
    "- Training_data (positive用)、文章数 : 700\n",
    "- Training_data (negative用)、文章数 : 700\n",
    "- Test_data (positive用)、文章数 : 3\n",
    "- Test_data (negative用)、文章数 : 3\n",
    "\n",
    "（ ※学習用データ：1400、　テスト用データ：6、　合計 : 1406 の文章です）\n",
    "\n",
    "`Training_data`を用いて機械学習を行い、その結果を元に、6つのTest dataがPositiveかNegativeかを判別してください。\n",
    "\n",
    "レッスン9で学んだ内容を踏まえ、各セルに'#コメント'の内容を実行するコードを記入してください。\n",
    "\n",
    "わからない場合は、ここまでのレッスン内容や各種ライブラリの公式ドキュメントを参照しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14e9b2d-e81d-4c46-aaa5-a21ead865efb"
    }
   },
   "source": [
    "## 1. 必要なモジュールの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Training_data` からデータファイルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_orig[neg]: loaded 700 e-mails.\n",
      "data_train[neg]:525   data_verify[neg]:175\n",
      "\n",
      "data_orig[pos]: loaded 700 e-mails.\n",
      "data_train[pos]:525   data_verify[pos]:175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training_data の読み込み\n",
    "datapath = Path('Amazon_review/Training_data')\n",
    "review_pattern = re.compile(r'cv\\d+')  # ファイル名が \"cv数字\"　で始まるファイル名かを調べる正規表現\n",
    "test_size = 0.25\n",
    "\n",
    "data_orig  = dict(neg=[], pos=[])\n",
    "data_train = dict(neg=[], pos=[])    # 学習データ\n",
    "data_verify  = dict(neg=[], pos=[])    # 検証データ\n",
    "\n",
    "# data_origへのデータの読み込み(training用ﾃﾞｰﾀ辞書化)\n",
    "for cls, mails in data_orig.items():\n",
    "    # data_orig内の各pos、negへのデータの読み込み\n",
    "    for path in (datapath / cls).iterdir():\n",
    "        if review_pattern.match(path.name):\n",
    "            with open(path, 'r', encoding='latin') as src:\n",
    "                mails.append(src.read())\n",
    "    print(f\"data_orig[{cls}]: loaded {len(mails)} e-mails.\")\n",
    "    \n",
    "    #自分用メモ：train/verify内の各pos、negへのデータの分割と格納\n",
    "    data_train[cls], data_verify[cls] = train_test_split(mails, test_size=test_size)\n",
    "    print(f\"data_train[{cls}]:{len(data_train[cls])}   data_verify[{cls}]:{len(data_verify[cls])}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じようにしてテスト用のファイルを読み込みます。`train_test_split` は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     neg: loaded 3 reviews.\n",
      "     pos: loaded 3 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Test_data の読み込み\n",
    "datapath_test = Path('Amazon_review/Test_data')\n",
    "review_test_pattern = re.compile(r'amazon_review_\\d+')  # ファイル名が \"amazon_review_数字\"　で始まるファイル名かを調べる正規表現\n",
    "data_test  = dict(neg=[], pos=[])    # テストデータ\n",
    "\n",
    "# data_testへのデータの読み込み(test用ﾃﾞｰﾀ辞書化)\n",
    "for cls, mails in data_test.items():\n",
    "    # 自分用メモ：data_test内の各pos、negへのデータの読み込み\n",
    "    for path in (datapath_test / cls).iterdir():\n",
    "        if review_test_pattern.match(path.name):\n",
    "            with open(path, 'r', encoding='latin') as src:\n",
    "                mails.append(src.read())\n",
    "    print(f\"{cls:>8}: loaded {len(mails)} reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの整形用の関数　get_values_and_targets　を定義する\n",
    "#自分用メモ：negとposの混合データvalues、そのbool解答targetの作成\n",
    "def get_values_and_targets(data):\n",
    "    values = data['neg'] + data['pos']\n",
    "    target = [True]*len(data['neg']) + [False]*len(data['pos'])\n",
    "    target = np.array(target)\n",
    "    return values, target\n",
    "\n",
    "# data_trainに対して get_values_and_targets を実行する\n",
    "'''\n",
    "自分用メモ：negとpos明示的に分かれた辞書data_trainから\n",
    "neg、pos混合の一次配列values_train(前半neg,後半pos)とその並びの\n",
    "一次配列bool型解答is_negative_train(前半True,後半False)を作成\n",
    "'''\n",
    "values_train, is_negative_train = get_values_and_targets(data_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negativeなテストデータのうち、最初のデータを表示してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iâve never been a Marvel Cinematic Universe fan; back in May 2012, the midnight premiere of The Avengers before I became a huge cinema buff, the only addition of the series I saw was Iron Man, which I felt no desire to watch again. I only bought a ticket to The Avengers because some friends invited me. Afterward, virtually the whole world went nuts and called it the greatest motion picture ever, I however just saw horrific, nauseous action void of artistic purpose. While softer attitudes towards Marvel eventually came my way, people praising ridiculous junk food over quality art still sickens me.\n",
      "\n",
      "Even today, Marvel representing the early bane of my taste in movies still affects my outlook upon Avengers: Infinity War. Like Rocket Raccoonâs mockery upon anybody different, Marvelâs corporate heads blare their red logo in its abused emotional manipulation, whilst their followers turn a deep azure hue in moral discouragement.\n",
      "\n",
      "This incorporation of many characters turns toilsome on a full bladder, since long stretches between subplots average between 20-40 minutes until returned to. The editors couldnât keep them consistently active, but sure, they got enough time for infinite out-of-place jokes! Thorâs first scene with the Guardians alone contains nonstop laughs increasing the total runtime threefold. Gags continue throughout serious beats, particularly one where Drax snacking on chips interrupts a sad romantic exchange.\n",
      "\n",
      "There are plentyâ¦ PLENTY of other flaws that ruin other evocative moments; whenever two combatants start punching and kicking, feeble battle choreographed by eye-sore camera movements looks set on anything besides vengeance. Rage seldom reaches full capacity, since no actors stayed on the same page: One aims to be optimistic (Chris Pratt), another aims to be deadpan (Robert Downey Jr.), and another cannot decide (Elizabeth Olsen), all cold in believability. Chadwick Boseman should take most of the blame as he continues his noncombative blank stares, alongside Chris Hemsworth as his obligatory smile disrupts any flow.\n",
      "\n",
      "Hereâs a clear perspective against the overhype of such a gimmicky unforeseen divergence from Marvel. Last Friday, Avengers: Infinity War skyrocketed up to #10 on IMDbâs Top 250, bumping up to #9 on Sunday (though itâs back at #10 now), surpassing genuinely good films: Saving Private Ryan, the Star Wars original trilogy, The Green Mile, Amadeus, and even Citizen Kane!\n",
      "\n",
      "These classics knew how to present new ideas in clever ways, something the Russo Brothers failed at. Nothing they did here is as clever as you would want to thinkâ¦ the climax alone rips off The Lord of the Rings, an enduring trilogy about healthy retaliation, in its grand epic scale. Now, I understand Marvel does honor Stan Leeâs created universe enough to allow greater depth than previous superhero establishments, except those corporate heads beneath Disneyâs control still insult the art of filmmaking by taking advantage of consumers for the sake of bank.\n",
      "\n",
      "Instead of futile cash-bait, imagine if we celebrated genuinely intelligent narrative commentaries on true problems? Or better yet, stopped hating on DC fanboys because of foolish loyalty to nonexistent people with abilities we could never hope to obtain? Then I can guarantee you that the world would become a much kinder place without the need for a dictatorial jerk in a dumb America costume telling us whatâs important in life.\n"
     ]
    }
   ],
   "source": [
    "# 読み込んだファイルの中身を表示（テストデータ, Negative, 1つ目）\n",
    "print(data_test[\"neg\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データの前処理と特徴ベクトルの作成\n",
    "\n",
    "次に `data_train` のデータを基に、特徴ベクトルを作成します。特徴ベクトルは `CountVectorizer` を使います。\n",
    "\n",
    "「記号の削除」と「3文字未満の単語の削除」も指定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actress',\n",
       " 'admire',\n",
       " 'adults',\n",
       " 'after',\n",
       " 'against',\n",
       " 'also',\n",
       " 'and',\n",
       " 'audience',\n",
       " 'awful',\n",
       " 'baby',\n",
       " 'bacharach',\n",
       " 'backdrop',\n",
       " 'bad',\n",
       " 'bargains',\n",
       " 'because',\n",
       " 'become',\n",
       " 'bell',\n",
       " 'bette',\n",
       " 'beyond',\n",
       " 'biggest',\n",
       " 'bit',\n",
       " 'blame',\n",
       " 'blow',\n",
       " 'blows',\n",
       " 'boomer',\n",
       " 'boy',\n",
       " 'burt',\n",
       " 'but',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cancer',\n",
       " 'character',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'credits',\n",
       " 'did',\n",
       " 'doesn',\n",
       " 'dolls',\n",
       " 'don',\n",
       " 'drugs',\n",
       " 'during',\n",
       " 'dying',\n",
       " 'earth',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everything',\n",
       " 'exploring',\n",
       " 'fail',\n",
       " 'fame',\n",
       " 'film',\n",
       " 'find',\n",
       " 'first',\n",
       " 'for',\n",
       " 'frankly',\n",
       " 'from',\n",
       " 'front',\n",
       " 'generally',\n",
       " 'gentile',\n",
       " 'getting',\n",
       " 'god',\n",
       " 'gratingly',\n",
       " 'greatness',\n",
       " 'grunt',\n",
       " 'hammy',\n",
       " 'have',\n",
       " 'hear',\n",
       " 'heavy',\n",
       " 'her',\n",
       " 'heroes',\n",
       " 'hollywood',\n",
       " 'hope',\n",
       " 'how',\n",
       " 'impossible',\n",
       " 'inspire',\n",
       " 'instantly',\n",
       " 'isn',\n",
       " 'its',\n",
       " 'jackie',\n",
       " 'jacqueline',\n",
       " 'jokes',\n",
       " 'just',\n",
       " 'kids',\n",
       " 'late',\n",
       " 'laughter',\n",
       " 'lead',\n",
       " 'life',\n",
       " 'like',\n",
       " 'little',\n",
       " 'loud',\n",
       " 'luck',\n",
       " 'lurid',\n",
       " 'maybe',\n",
       " 'midler',\n",
       " 'might',\n",
       " 'model',\n",
       " 'more',\n",
       " 'movie',\n",
       " 'name',\n",
       " 'not',\n",
       " 'novel',\n",
       " 'novelist',\n",
       " 'now',\n",
       " 'obnoxious',\n",
       " 'oddly',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'opening',\n",
       " 'our',\n",
       " 'penned',\n",
       " 'person',\n",
       " 'plain',\n",
       " 'play',\n",
       " 'prays',\n",
       " 'preferred',\n",
       " 'qualifies',\n",
       " 'quite',\n",
       " 'radio',\n",
       " 'really',\n",
       " 'relate',\n",
       " 'ring',\n",
       " 'rise',\n",
       " 'role',\n",
       " 'said',\n",
       " 'saying',\n",
       " 'score',\n",
       " 'selling',\n",
       " 'sex',\n",
       " 'she',\n",
       " 'shocking',\n",
       " 'showing',\n",
       " 'smiled',\n",
       " 'star',\n",
       " 'start',\n",
       " 'stunned',\n",
       " 'susann',\n",
       " 'table',\n",
       " 'tagline',\n",
       " 'tale',\n",
       " 'talent',\n",
       " 'ten',\n",
       " 'tepid',\n",
       " 'that',\n",
       " 'the',\n",
       " 'them',\n",
       " 'think',\n",
       " 'this',\n",
       " 'three',\n",
       " 'throughout',\n",
       " 'time',\n",
       " 'tinny',\n",
       " 'too',\n",
       " 'trashy',\n",
       " 'tree',\n",
       " 'two',\n",
       " 'valley',\n",
       " 'was',\n",
       " 'washed',\n",
       " 'way',\n",
       " 'went',\n",
       " 'what',\n",
       " 'when',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whore',\n",
       " 'why',\n",
       " 'with',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'would',\n",
       " 'wretches',\n",
       " 'years',\n",
       " 'you',\n",
       " 'young']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの前処理を指定して、特徴量ベクトルを作成\n",
    "vocab = CountVectorizer(token_pattern=r'[a-zA-Z]{3,}').fit([data_train['neg'][0]])\n",
    "vocab.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t2\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 6)\t12\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t2\n",
      "  (0, 16)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t2\n",
      "  (0, 21)\t1\n",
      "  (0, 22)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 24)\t1\n",
      "  :\t:\n",
      "  (0, 150)\t1\n",
      "  (0, 151)\t1\n",
      "  (0, 152)\t1\n",
      "  (0, 153)\t3\n",
      "  (0, 154)\t1\n",
      "  (0, 155)\t1\n",
      "  (0, 156)\t2\n",
      "  (0, 157)\t6\n",
      "  (0, 158)\t1\n",
      "  (0, 159)\t3\n",
      "  (0, 160)\t1\n",
      "  (0, 161)\t3\n",
      "  (0, 162)\t2\n",
      "  (0, 163)\t1\n",
      "  (0, 164)\t1\n",
      "  (0, 165)\t1\n",
      "  (0, 166)\t1\n",
      "  (0, 167)\t4\n",
      "  (0, 168)\t1\n",
      "  (0, 169)\t1\n",
      "  (0, 170)\t2\n",
      "  (0, 171)\t1\n",
      "  (0, 172)\t1\n",
      "  (0, 173)\t4\n",
      "  (0, 174)\t1\n"
     ]
    }
   ],
   "source": [
    "# print() で特徴量データを表示\n",
    "print(vocab.transform([data_train[\"neg\"][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量を配列形式で表示\n",
    "vocab    = CountVectorizer(binary=True, token_pattern=r'[a-zA-Z]{3,}')\n",
    "features = vocab.fit_transform(values_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 30927)\n"
     ]
    }
   ],
   "source": [
    "# 特徴量のshapeを確認\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. フィルタアルゴリズムの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロジスティックモデルを使ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロジスティックモデルを採用してフィッティングを行う\n",
    "model = LogisticRegression(solver='saga', max_iter=1000, random_state=539167).fit(features, is_negative_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_verify の特徴量ベクトルを作成し、モデルに分類させてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_verifyに対して get_values_and_targets を実行する\n",
    "'''\n",
    "自分用メモ：テスト用辞書data_verify(negとpos明示的に分かれたもの)から\n",
    "neg、pos混合の一次配列values_test(前半neg,後半pos)とその並びの\n",
    "一次配列bool型解答is_negative_test(前半True,後半False)を作成\n",
    "'''\n",
    "values_test, is_negative_test = get_values_and_targets(data_verify)\n",
    "\n",
    "# モデルに分類させる\n",
    "'''\n",
    "自分用メモ：Training_data内ﾄﾚｰﾆﾝｸﾞﾃﾞｰﾀ(7.5割)を元に作成したﾓﾃﾞﾙで\n",
    "Training_data内検証ﾃﾞｰﾀ(2.5割)のvalues_test(前半neg,後半pos)を\n",
    "transform(特徴量配列形式化)してﾛｼﾞｽﾃｨｯｸﾓﾃﾞﾙpredictにかける\n",
    "'''\n",
    "pred_test = model.predict(vocab.transform(values_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正答率を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/350 correct (81.429%)\n"
     ]
    }
   ],
   "source": [
    "# 正答率を確認する\n",
    "#自分用メモ：Training_data内検証ﾃﾞｰﾀ(2.5割)の正解率\n",
    "validation = (pred_test == is_negative_test)\n",
    "size = validation.size\n",
    "correct = np.count_nonzero(validation)\n",
    "print(f\"{correct}/{size} correct ({correct*100/size:.3f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. テストデータで予測を実行する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証データを使った検証の結果、比較的高い正答率が確認できたと思います。\n",
    "\n",
    "最後に、data_test の **カテゴリごとに** 特徴量ベクトルを作成し、モデルに分類させてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     neg: 145/175 correct (82.857%)\n",
      "     pos: 140/175 correct (80.000%)\n"
     ]
    }
   ],
   "source": [
    "# data_testのカテゴリごとに正答率を確認する\n",
    "for char in (\"neg\",\"pos\"):\n",
    "    '''\n",
    "    自分用メモ：\n",
    "    ・辞書ｷｰ毎の情報読み込み\n",
    "    ・bool解答の配列作成（negはTrue、posはFalse 繰り返しの中で、全てTrueか全てFalseとなる）\n",
    "    ・Training_data内ﾄﾚｰﾆﾝｸﾞﾃﾞｰﾀ(7.5割)で作成したﾓﾃﾞﾙでposのみかnegのみの予想作成\n",
    "    ・予想と解答をつき合わせ、合っていれば(True=TrueかFalse=False)Trueとする正誤配列作成\n",
    "    ・配列のサイズ＝ﾃｽﾄﾃﾞｰﾀ個数を取得\n",
    "    ・正誤配列からnotFalse(True)のみを読み出し、正解数を算出\n",
    "    '''\n",
    "    _valuse = data_verify[char]\n",
    "    _is_negative = [(char == \"neg\")]*len(_valuse)\n",
    "    _pred = model.predict(vocab.transform(_valuse))\n",
    "    _valid = (_pred == _is_negative)\n",
    "    _size = _valid.size\n",
    "    _correct = np.count_nonzero(_valid)\n",
    "    print(f\"{char:>8s}: {_correct:>3d}/{_size:>3d} correct ({_correct*100/_size:.3f}%)\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
